{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Importing-the-libraries-游빓\" data-toc-modified-id=\"Importing-the-libraries-游빓-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Importing the libraries 游빓</a></span></li><li><span><a href=\"#Loading-the-datasets-游니\" data-toc-modified-id=\"Loading-the-datasets-游니-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading the datasets 游니</a></span></li><li><span><a href=\"#NBM\" data-toc-modified-id=\"NBM-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>NBM</a></span></li><li><span><a href=\"#LSVC\" data-toc-modified-id=\"LSVC-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>LSVC</a></span></li><li><span><a href=\"#Logreg\" data-toc-modified-id=\"Logreg-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Logreg</a></span></li><li><span><a href=\"#Ridge\" data-toc-modified-id=\"Ridge-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ridge</a></span></li><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Hyperparameter Tuning</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Climate Change Belief Challenge 游깴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries 游빓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import various packages and modules which enable us to pefore many different tasks, such as manipulating the data, viewing the data, performing calculations of the data and visualising the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\court\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\court\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\court\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk \n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import wordcloud\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "import stopwordsiso as stopwordz\n",
    "from wordcloud import WordCloud \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "words = set(nltk.corpus.words.words())\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets 游니"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now load the data we will be using. The data we have recieved is in .csv (comma separated values) format, so to access the data through python, we use pandas (pd.read).We have 2 files, split into df_test and df_train data. The df_test data is a subset of df_train, and we use this method to ensure that the models we build are safe from data leakage and therefore, are of better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('https://github.com/RickieMohale/classification-predict-streamlit-template/blob/master/train.csv?raw=true')\n",
    "df_test = pd.read_csv('https://github.com/RickieMohale/classification-predict-streamlit-template/blob/master/test_with_no_labels.csv?raw=true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['token'] = df_train['message'].apply(nltk.word_tokenize)\n",
    "df_test['token'] = df_test['message'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['token'] = df_train['message'].apply(nltk.word_tokenize)\n",
    "df_test['token'] = df_test['message'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['token'] = df_train['token'].apply(lambda x : [token for token in x if token not in string.punctuation])\n",
    "df_test['token'] = df_test['token'].apply(lambda x : [token for token in x if token not in string.punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing single character tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['token'] = df_train['token'].apply(lambda x: [token for token in x if len(token) > 1])\n",
    "df_test['token'] = df_test['token'].apply(lambda x: [token for token in x if len(token) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stopwordsiso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['no_stopwords'] = df_train['token'].apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords for df_train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords for df_test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_eng = stopwords.words('english')\n",
    "df_train['no_stopwords'] = df_train['token'].apply(lambda x: [item for item in x if item not in stop_eng])\n",
    "df_test['no_stopwords'] = df_test['token'].apply(lambda x: [item for item in x if item not in stop_eng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>token</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[PolySciMajor, EPA, chief, does, n't, think, c...</td>\n",
       "      <td>[PolySciMajor, EPA, chief, n't, think, carbon,...</td>\n",
       "      <td>[PolySciMajor, EPA, chief, n't, think, carbon,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[It, 's, not, like, we, lack, evidence, of, an...</td>\n",
       "      <td>[It, 's, like, lack, evidence, anthropogenic, ...</td>\n",
       "      <td>[It, 's, like, lack, evidence, anthropogenic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[RT, RawStory, Researchers, say, we, have, thr...</td>\n",
       "      <td>[RT, RawStory, Researchers, say, three, years,...</td>\n",
       "      <td>[RT, RawStory, Researchers, say, three, year, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[TodayinMaker, WIRED, 2016, was, pivotal, year...</td>\n",
       "      <td>[TodayinMaker, WIRED, 2016, pivotal, year, war...</td>\n",
       "      <td>[TodayinMaker, WIRED, 2016, pivotal, year, war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[RT, SoyNovioDeTodas, It, 's, 2016, and, racis...</td>\n",
       "      <td>[RT, SoyNovioDeTodas, It, 's, 2016, racist, se...</td>\n",
       "      <td>[RT, SoyNovioDeTodas, It, 's, 2016, racist, se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221   \n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562   \n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736   \n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954   \n",
       "\n",
       "                                               token  \\\n",
       "0  [PolySciMajor, EPA, chief, does, n't, think, c...   \n",
       "1  [It, 's, not, like, we, lack, evidence, of, an...   \n",
       "2  [RT, RawStory, Researchers, say, we, have, thr...   \n",
       "3  [TodayinMaker, WIRED, 2016, was, pivotal, year...   \n",
       "4  [RT, SoyNovioDeTodas, It, 's, 2016, and, racis...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  [PolySciMajor, EPA, chief, n't, think, carbon,...   \n",
       "1  [It, 's, like, lack, evidence, anthropogenic, ...   \n",
       "2  [RT, RawStory, Researchers, say, three, years,...   \n",
       "3  [TodayinMaker, WIRED, 2016, pivotal, year, war...   \n",
       "4  [RT, SoyNovioDeTodas, It, 's, 2016, racist, se...   \n",
       "\n",
       "                                                 lem  \n",
       "0  [PolySciMajor, EPA, chief, n't, think, carbon,...  \n",
       "1  [It, 's, like, lack, evidence, anthropogenic, ...  \n",
       "2  [RT, RawStory, Researchers, say, three, year, ...  \n",
       "3  [TodayinMaker, WIRED, 2016, pivotal, year, war...  \n",
       "4  [RT, SoyNovioDeTodas, It, 's, 2016, racist, se...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df_train['lem'] = df_train['no_stopwords'].apply(lambda x: lemmatizer(x))\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>token</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>[Europe, will, now, be, looking, to, China, to...</td>\n",
       "      <td>[Europe, looking, China, make, sure, alone, fi...</td>\n",
       "      <td>[Europe, looking, China, make, sure, alone, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>[Combine, this, with, the, polling, of, staffe...</td>\n",
       "      <td>[Combine, polling, staffers, climate, change, ...</td>\n",
       "      <td>[Combine, polling, staffer, climate, change, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "      <td>[The, scary, unimpeachable, evidence, that, cl...</td>\n",
       "      <td>[The, scary, unimpeachable, evidence, climate,...</td>\n",
       "      <td>[The, scary, unimpeachable, evidence, climate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "      <td>[Karoli, morgfair, OsborneInk, dailykos, Putin...</td>\n",
       "      <td>[Karoli, morgfair, OsborneInk, dailykos, Putin...</td>\n",
       "      <td>[Karoli, morgfair, OsborneInk, dailykos, Putin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "      <td>[RT, FakeWillMoore, 'Female, orgasms, cause, g...</td>\n",
       "      <td>[RT, FakeWillMoore, 'Female, orgasms, cause, g...</td>\n",
       "      <td>[RT, FakeWillMoore, 'Female, orgasm, cause, gl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  Europe will now be looking to China to make su...   169760   \n",
       "1  Combine this with the polling of staffers re c...    35326   \n",
       "2  The scary, unimpeachable evidence that climate...   224985   \n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263   \n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928   \n",
       "\n",
       "                                               token  \\\n",
       "0  [Europe, will, now, be, looking, to, China, to...   \n",
       "1  [Combine, this, with, the, polling, of, staffe...   \n",
       "2  [The, scary, unimpeachable, evidence, that, cl...   \n",
       "3  [Karoli, morgfair, OsborneInk, dailykos, Putin...   \n",
       "4  [RT, FakeWillMoore, 'Female, orgasms, cause, g...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  [Europe, looking, China, make, sure, alone, fi...   \n",
       "1  [Combine, polling, staffers, climate, change, ...   \n",
       "2  [The, scary, unimpeachable, evidence, climate,...   \n",
       "3  [Karoli, morgfair, OsborneInk, dailykos, Putin...   \n",
       "4  [RT, FakeWillMoore, 'Female, orgasms, cause, g...   \n",
       "\n",
       "                                                 lem  \n",
       "0  [Europe, looking, China, make, sure, alone, fi...  \n",
       "1  [Combine, polling, staffer, climate, change, w...  \n",
       "2  [The, scary, unimpeachable, evidence, climate,...  \n",
       "3  [Karoli, morgfair, OsborneInk, dailykos, Putin...  \n",
       "4  [RT, FakeWillMoore, 'Female, orgasm, cause, gl...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df_test['lem'] = df_test['no_stopwords'].apply(lambda x: lemmatizer(x))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating X, y and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>token</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>[Europe, will, now, be, looking, to, China, to...</td>\n",
       "      <td>[Europe, looking, China, make, sure, alone, fi...</td>\n",
       "      <td>[Europe, looking, China, make, sure, alone, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>[Combine, this, with, the, polling, of, staffe...</td>\n",
       "      <td>[Combine, polling, staffers, climate, change, ...</td>\n",
       "      <td>[Combine, polling, staffer, climate, change, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "      <td>[The, scary, unimpeachable, evidence, that, cl...</td>\n",
       "      <td>[The, scary, unimpeachable, evidence, climate,...</td>\n",
       "      <td>[The, scary, unimpeachable, evidence, climate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "      <td>[Karoli, morgfair, OsborneInk, dailykos, Putin...</td>\n",
       "      <td>[Karoli, morgfair, OsborneInk, dailykos, Putin...</td>\n",
       "      <td>[Karoli, morgfair, OsborneInk, dailykos, Putin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "      <td>[RT, FakeWillMoore, 'Female, orgasms, cause, g...</td>\n",
       "      <td>[RT, FakeWillMoore, 'Female, orgasms, cause, g...</td>\n",
       "      <td>[RT, FakeWillMoore, 'Female, orgasm, cause, gl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  Europe will now be looking to China to make su...   169760   \n",
       "1  Combine this with the polling of staffers re c...    35326   \n",
       "2  The scary, unimpeachable evidence that climate...   224985   \n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263   \n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928   \n",
       "\n",
       "                                               token  \\\n",
       "0  [Europe, will, now, be, looking, to, China, to...   \n",
       "1  [Combine, this, with, the, polling, of, staffe...   \n",
       "2  [The, scary, unimpeachable, evidence, that, cl...   \n",
       "3  [Karoli, morgfair, OsborneInk, dailykos, Putin...   \n",
       "4  [RT, FakeWillMoore, 'Female, orgasms, cause, g...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  [Europe, looking, China, make, sure, alone, fi...   \n",
       "1  [Combine, polling, staffers, climate, change, ...   \n",
       "2  [The, scary, unimpeachable, evidence, climate,...   \n",
       "3  [Karoli, morgfair, OsborneInk, dailykos, Putin...   \n",
       "4  [RT, FakeWillMoore, 'Female, orgasms, cause, g...   \n",
       "\n",
       "                                                 lem  \n",
       "0  [Europe, looking, China, make, sure, alone, fi...  \n",
       "1  [Combine, polling, staffer, climate, change, w...  \n",
       "2  [The, scary, unimpeachable, evidence, climate,...  \n",
       "3  [Karoli, morgfair, OsborneInk, dailykos, Putin...  \n",
       "4  [RT, FakeWillMoore, 'Female, orgasm, cause, gl...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df_test['lem'] = df_test['no_stopwords'].apply(lambda x: lemmatizer(x))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['message']\n",
    "y = df_train['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80% of the df_train set to df_train the model, 20% to validate.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = list(X_train.apply(' '.join))\n",
    "X_val = list(X_val.apply(' '.join))\n",
    "X_test = list(X_test.apply(' '.join))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Multinomial\n",
    "nbm_tfidf = Pipeline([('tfidf', TfidfVectorizer()),('nbm', MultinomialNB()),])\n",
    "nbm_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             max_features = 180000,\n",
    "                             min_df = 1,\n",
    "                             ngram_range = (1,3)\n",
    "                            )),('nbm', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('CountVec',\n",
       "                 CountVectorizer(max_features=180000, ngram_range=(1, 3))),\n",
       "                ('nbm', MultinomialNB())])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbm_tfidf.fit(X_train, y_train)\n",
    "nbm_count.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC Pipeline\n",
    "Lsvc_tfidf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('scv', LinearSVC()),])\n",
    "Lsvc_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             max_features = 180000,\n",
    "                             min_df = 1,\n",
    "                             ngram_range = (1,3)\n",
    "                            )),('svc', LinearSVC()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\court\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('CountVec',\n",
       "                 CountVectorizer(max_features=180000, ngram_range=(1, 3))),\n",
       "                ('svc', LinearSVC())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lsvc_tfidf.fit(X_train, y_train)\n",
    "Lsvc_count.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression pipeline\n",
    "logreg_tfidf = Pipeline([('tfidf', TfidfVectorizer()),('logistic', LogisticRegression()),])\n",
    "logreg_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             max_features = 180000,\n",
    "                             min_df = 1,\n",
    "                             ngram_range = (1,3)\n",
    "                            )),('logistic', LogisticRegression()),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\court\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\court\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('CountVec',\n",
       "                 CountVectorizer(max_features=180000, ngram_range=(1, 3))),\n",
       "                ('logistic', LogisticRegression())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_tfidf.fit(X_train, y_train)\n",
    "logreg_count.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Pipeline\n",
    "ridge_tfidf = Pipeline([('tfidf', TfidfVectorizer()), ('Ridge', RidgeClassifier())])\n",
    "ridge_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             max_features = 180000,\n",
    "                             min_df = 1,\n",
    "                             ngram_range = (1,3)\n",
    "                            )),('Ridge', RidgeClassifier()),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('CountVec',\n",
       "                 CountVectorizer(max_features=180000, ngram_range=(1, 3))),\n",
       "                ('Ridge', RidgeClassifier())])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the Ridge classifier\n",
    "ridge_tfidf.fit(X_train, y_train)\n",
    "ridge_count.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline for the gridsearch\n",
    "param_grid= {'C': [0.01, 0.03, 0.06, 0.09]}  # setting parameter grid\n",
    "\n",
    "tuned = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n",
    "                                                max_df=0.9,\n",
    "                                                ngram_range=(1, 2))),\n",
    "                      ('mnb', GridSearchCV(LinearSVC(),\n",
    "                                           param_grid=param_grid,\n",
    "                                           cv=10,\n",
    "                                           n_jobs=-1,\n",
    "                                           scoring='f1_weighted'))\n",
    "                      ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.9, ngram_range=(1, 2))),\n",
       "                ('mnb',\n",
       "                 GridSearchCV(cv=10, estimator=LinearSVC(), n_jobs=-1,\n",
       "                              param_grid={'C': [0.01, 0.03, 0.06, 0.09]},\n",
       "                              scoring='f1_weighted'))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions = Lsvc_tfidf.predict(df_test[\"message\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ztest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-36bea463082a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m kaggle_df = pd.DataFrame(\n\u001b[1;32m----> 2\u001b[1;33m     {'tweetid': df_ztest['tweetid'],\n\u001b[0m\u001b[0;32m      3\u001b[0m      \u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkaggle_predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_ztest' is not defined"
     ]
    }
   ],
   "source": [
    "kaggle_df = pd.DataFrame(\n",
    "    {'tweetid': df_ztest['tweetid'],\n",
    "     'sentiment': kaggle_predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df.to_csv(\"tuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pkl 4 lsvc tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"Lsvc_tfidf.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(Lsvc_tfidf,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pkl 4 ridge tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"ridge_tfidf.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(ridge_tfidf,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pkl 4 tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"tuned.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(tuned,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
